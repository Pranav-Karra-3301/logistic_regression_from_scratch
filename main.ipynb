{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy import log,dot,e,shape\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "import numpy as np \n",
    "from numpy import log,dot,e,shape\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Sklearn's make_classification dataset with four features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4) (10, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X,y = make_classification(n_features = 4,n_classes=2)\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.1)\n",
    "\n",
    "print(X_tr.shape, X_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X_tr):\n",
    "    for i in range(shape(X_tr)[1]):\n",
    "        X_tr[:,i] = (X_tr[:,i] - np.mean(X_tr[:,i]))/np.std(X_tr[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(self,X):\n",
    "    weights = np.zeros((shape(X)[1]+1,1))\n",
    "    X = np.c_[np.ones((shape(X)[0],1)),X]\n",
    "    return weights,X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \n",
    "    def initialize(self,X):\n",
    "        weights = np.zeros((shape(X)[1]+1,1))\n",
    "        X = np.c_[np.ones((shape(X)[0],1)),X]\n",
    "        return weights,X\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        sig = 1/(1+e**(-z))\n",
    "        return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta):\n",
    "    z = dot(X,theta)\n",
    "    cost0 = y.T.dot(log(self.sigmoid(z)))\n",
    "    cost1 = (1-y).T.dot(log(1-self.sigmoid(z)))\n",
    "    cost = -((cost1 + cost0))/len(y) \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Gradient Descent to find Best Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LogisticRegression)\n",
    "    def fit(self,X,y,alpha=0.001,iter=100):\n",
    "            params,X = self.initialize(X)\n",
    "            cost_list = np.zeros(iter,)\n",
    "            for i in range(iter):\n",
    "                params = params - alpha * dot(X.T, self.sigmoid(dot(X,params)) - np.reshape(y,(len(y),1)))\n",
    "                cost_list[i] = cost(params)\n",
    "            self.params = params\n",
    "            return cost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y,y_hat):\n",
    "    tp,tn,fp,fn = 0,0,0,0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1 and y_hat[i] == 1:\n",
    "            tp += 1\n",
    "        elif y[i] == 1 and y_hat[i] == 0:\n",
    "            fn += 1\n",
    "        elif y[i] == 0 and y_hat[i] == 1:\n",
    "            fp += 1\n",
    "        elif y[i] == 0 and y_hat[i] == 0:\n",
    "            tn += 1\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the code together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9397590361445782\n",
      "0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13647/177943105.py:54: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  cost_list[i] = cost(weights)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from numpy import log,dot,exp,shape\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_features=4, n_classes=2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.1)\n",
    "\n",
    "def standardize(X_tr):\n",
    "\n",
    "    for i in range(shape(X_tr)[1]):\n",
    "        X_tr[:,i] = (X_tr[:,i] - np.mean(X_tr[:,i]))/np.std(X_tr[:,i])\n",
    "\n",
    "def F1_score(y,y_hat):\n",
    "\n",
    "    tp,tn,fp,fn = 0,0,0,0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1 and y_hat[i] == 1:\n",
    "            tp += 1\n",
    "        elif y[i] == 1 and y_hat[i] == 0:\n",
    "            fn += 1\n",
    "        elif y[i] == 0 and y_hat[i] == 1:\n",
    "            fp += 1\n",
    "        elif y[i] == 0 and y_hat[i] == 0:\n",
    "            tn += 1\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    return f1_score\n",
    "\n",
    "class LogidticRegression:\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        sig = 1/(1+exp(-z))\n",
    "        return sig\n",
    "    def initialize(self,X):\n",
    "        weights = np.zeros((shape(X)[1]+1,1))\n",
    "        X = np.c_[np.ones((shape(X)[0],1)),X]\n",
    "        return weights,X\n",
    "    def fit(self,X,y,alpha=0.001,iter=400):\n",
    "        weights,X = self.initialize(X)\n",
    "        def cost(theta):\n",
    "            z = dot(X,theta)\n",
    "            cost0 = y.T.dot(log(self.sigmoid(z)))\n",
    "            cost1 = (1-y).T.dot(log(1-self.sigmoid(z)))\n",
    "            cost = -((cost1 + cost0))/len(y)\n",
    "            return cost\n",
    "        cost_list = np.zeros(iter,)\n",
    "        for i in range(iter):\n",
    "            weights = weights - alpha*dot(X.T,self.sigmoid(dot(X,weights))-np.reshape(y,(len(y),1)))\n",
    "            cost_list[i] = cost(weights)\n",
    "        self.weights = weights\n",
    "        return cost_list\n",
    "    def predict(self,X):\n",
    "        z = dot(self.initialize(X)[1],self.weights)\n",
    "        lis = []\n",
    "        for i in self.sigmoid(z):\n",
    "            if i>0.5:\n",
    "                lis.append(1)\n",
    "            else:\n",
    "                lis.append(0)\n",
    "        return lis\n",
    "    \n",
    "standardize(X_tr)\n",
    "standardize(X_te)\n",
    "obj1 = LogidticRegression()\n",
    "model= obj1.fit(X_tr,y_tr)\n",
    "y_pred = obj1.predict(X_te)\n",
    "y_train = obj1.predict(X_tr)\n",
    "#Let's see the f1-score for training and testing data\n",
    "f1_score_tr = F1_score(y_tr,y_train)\n",
    "f1_score_te = F1_score(y_te,y_pred)\n",
    "print(f1_score_tr)\n",
    "print(f1_score_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing this Logistic Regression with that of Sklearn's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "model = LogisticRegression().fit(X_tr,y_tr)\n",
    "y_pred = model.predict(X_te)\n",
    "print(f1_score(y_te,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Works pretty well! üëç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4) (10, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X,y = make_classification(n_features = 4,n_classes=2)\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.1)\n",
    "\n",
    "print(X_tr.shape, X_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
